* Introduction

This is my personal implementation for several algorithms, some of which are cutting edge, including
1. [[https://arxiv.org/abs/1312.5602][DQN]]
2. [[https://arxiv.org/abs/1509.02971][DDPG]]
3. [[https://arxiv.org/abs/1602.01783][A3C]]
4. [[http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf][REINFORCE]]

Some optimizations are used. [[https://arxiv.org/abs/1509.06461][Double DQN]] is implemented instead of traditional DQN.
Furthermore, [[https://arxiv.org/abs/1511.05952][prioritized sampling]] is currently being developing.

Lots of codes are ad-hoc and needs refactored. Issues and discussions are always appreciated.

* Tests

Few examples are available now, due to lots of bugs. However, DDPG may succeed now. All codes depend on OpenAI/gym and TensorFlow, so if you want to run any experiments, install them please.

#+BEGIN_SRC bash
    mkdir -p models/MountainCarContinuous-v0
    python main.py --mode train --env MountainCarContinuous-v0 --agent DDPG
#+END_SRC

Implementation for REINFORCE:

#+BEGIN_SRC bash
    python mountaincar-bak.py --mode train --env CartPole-v0
#+END_SRC

Experimental A2C (*synchronous* advantage actor-critic) running on CartPole-v0. Note that A2C uses LSTM by default.

#+BEGIN_SRC bash
    main.py --env CartPole-v0 --mode train --agent A2C --learning_rate 0.001 --replay_buffer_size 200 --batch_size 200 --t_max 5 --num_units 50
#+END_SRC