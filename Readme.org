* Introduction

This is my personal implementation for several algorithms, some of which are cutting edge, including
1. [[https://arxiv.org/abs/1312.5602][DQN]]
2. [[https://arxiv.org/abs/1509.02971][DDPG]]
3. [[https://arxiv.org/abs/1602.01783][A3C]]
4. [[http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf][REINFORCE]]

Some optimizations are used. [[https://arxiv.org/abs/1509.06461][Double DQN]] is implemented instead of traditional DQN.
Furthermore, [[https://arxiv.org/abs/1511.05952][prioritized sampling]] is currently being developing.

Lots of codes are ad-hoc and needs refactored. Issues and discussions are always appreciated.

* Tests

Few examples are available now, due to lots of bugs. However, DDPG may succeed now. All codes depend on OpenAI/gym and TensorFlow, so if you want to run any experiments, install them please.

#+BEGIN_SRC bash
    mkdir -p models/MountainCarContinuous-v0
    python MountainCarContinuous.py --mode train --env MountainCarContinuous-v0
#+END_SRC

Implementation for REINFORCE:

#+BEGIN_SRC bash
    python mountaincar-bak.py --mode train --env CartPole-v0
#+END_SRC

Experimental A2C (*synchronous* advantage actor-critic) running on CartPole-v0. Note that A2C uses LSTM by default.

#+BEGIN_SRC bash
    python Copy-v0.py --env CartPole-v0 --mode train --iterations 100000 --GAE-decay 1 --gamma 0.99 --tau 0.1 --batch-size 10
#+END_SRC